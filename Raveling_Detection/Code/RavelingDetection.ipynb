{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-02-09T13:54:40.782633Z",
     "iopub.status.busy": "2022-02-09T13:54:40.782271Z",
     "iopub.status.idle": "2022-02-09T13:54:41.050439Z",
     "shell.execute_reply": "2022-02-09T13:54:41.049317Z",
     "shell.execute_reply.started": "2022-02-09T13:54:40.782547Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# [Raveling Detection Challenge](https://www.kaggle.com/c/raveling-detection-ce784a-2022/data)\n",
    "## Author : Shivam Pandey [@ShivamPR21](https://shivampr21.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules for data manipulation\n",
    "1. OS : Python\n",
    "2. Numpy : Matrix manipulation library\n",
    "3. Pandas : Tabular dataset manipulation, read, and write\n",
    "4. Pillow.Image : Image read write modality\n",
    "5. Scipy.stats : Stastical properties of data\n",
    "6. skimages : Image manipuation, and feature extraction\n",
    "7. matplotlib.pyplot : data plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T05:54:01.461616Z",
     "iopub.status.busy": "2022-02-14T05:54:01.461329Z",
     "iopub.status.idle": "2022-02-14T05:54:02.746532Z",
     "shell.execute_reply": "2022-02-14T05:54:02.745666Z",
     "shell.execute_reply.started": "2022-02-14T05:54:01.461585Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from PIL import Image\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import img_as_ubyte\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Pytorch\n",
    "- Useful for deep learning model creation, training, and dataset handeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T05:54:02.748227Z",
     "iopub.status.busy": "2022-02-14T05:54:02.747997Z",
     "iopub.status.idle": "2022-02-14T05:54:14.476597Z",
     "shell.execute_reply": "2022-02-14T05:54:14.475771Z",
     "shell.execute_reply.started": "2022-02-14T05:54:02.748197Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "try:\n",
    "    from torchsummary import summary\n",
    "except:\n",
    "    print(\"Installing Torchsummary..........\")\n",
    "    ! pip install torchsummary\n",
    "    from torchsummary import summary\n",
    "\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset preprocessing and split modality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T05:54:14.478988Z",
     "iopub.status.busy": "2022-02-14T05:54:14.478476Z",
     "iopub.status.idle": "2022-02-14T05:54:14.639358Z",
     "shell.execute_reply": "2022-02-14T05:54:14.638713Z",
     "shell.execute_reply.started": "2022-02-14T05:54:14.478944Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create `RavelingTrainDataset` class\n",
    "1. Handels Image manipulation, train, and test datasets\n",
    "2. Splits the training dataset into validation and trainning sets.\n",
    "3. Normalizes the datasets with `RobustScaler` method\n",
    "4. Creats the tabular feature dataset __Required for ANNs__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T06:11:41.210508Z",
     "iopub.status.busy": "2022-02-14T06:11:41.209545Z",
     "iopub.status.idle": "2022-02-14T06:11:41.241969Z",
     "shell.execute_reply": "2022-02-14T06:11:41.241024Z",
     "shell.execute_reply.started": "2022-02-14T06:11:41.210441Z"
    }
   },
   "outputs": [],
   "source": [
    "class RavelingTrainDataset():\n",
    "    \n",
    "    def __init__(self, \n",
    "                 root = '/kaggle/input/raveling-detection-ce784a-2022/mod_ravelling_dataset',\n",
    "                 train_test_dirs = ['train', 'test'],\n",
    "                 classes = ['Non_raveling', 'Raveling'],\n",
    "                 val_fraction = 0.2,\n",
    "                 random_state = 42):\n",
    "        self.root = os.path.join(root, train_test_dirs[0]) # Root Dir\n",
    "        self.test_root = os.path.join(root, train_test_dirs[1]) # Test Root dir\n",
    "        self.classes = classes # classes name list\n",
    "        self.data_file_path = {}\n",
    "        self.test_data_path = []\n",
    "        self.data_x = [] # Complete data features\n",
    "        self.data_y = [] # Complete data label\n",
    "        self.data_train_x = [] # Train data features\n",
    "        self.data_train_y = [] # Train data label\n",
    "        self.scaler = RobustScaler() # Robust scaler instance to scale the data\n",
    "        \n",
    "        self.data_test = [] # Test data\n",
    "        self.data_val_x = [] # validation data features\n",
    "        self.data_val_y = [] # validation data label\n",
    "        \n",
    "        self.val_fraction = val_fraction # Validation fraction\n",
    "        self.random_state = random_state # Random state to regenerate the samples\n",
    "        \n",
    "        self.generate_data_file_path() # Generate the data file path\n",
    "        self.generate_tabular_data() # Extract features and create the tabular form of data\n",
    "        self.load_test_data() # Load the test dataset and create tabular form\n",
    "        \n",
    "    def generate_data_file_path(self):\n",
    "        for c in self.classes:\n",
    "            self.data_file_path.update({c : [\n",
    "                os.path.join(self.root, c, x) for x in \\\n",
    "                os.listdir(os.path.join(self.root, c))]\n",
    "                                       })\n",
    "            \n",
    "    def generate_features(self, file_path):\n",
    "        \"\"\"\n",
    "        Given file path the function returns extracted features list\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        \n",
    "        img = Image.open(file_path)\n",
    "        img = np.asarray(img)\n",
    "        \n",
    "        for i in range(3):\n",
    "            tmp = img[:, :, i].flatten()\n",
    "            features.extend([np.mean(tmp), \n",
    "                             np.std(tmp), \n",
    "                             skew(tmp), \n",
    "                             kurtosis(tmp), \n",
    "                             np.max(tmp) - np.min(tmp)])\n",
    "        \n",
    "        img_gray = img_as_ubyte(rgb2gray(img))\n",
    "        glcm = graycomatrix(img_gray, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4], 256, \n",
    "                            symmetric=True, normed=True)\n",
    "        dissimilarity = graycoprops(glcm, 'dissimilarity')\n",
    "        correlation = graycoprops(glcm, 'correlation')\n",
    "        homogeneity = graycoprops(glcm,'homogeneity')\n",
    "        energy = graycoprops(glcm, 'energy')\n",
    "        contrast = graycoprops(glcm, 'contrast')\n",
    "        ASM = graycoprops(glcm,'ASM')\n",
    "        \n",
    "        for glcm_feature in [dissimilarity, correlation, homogeneity, energy, contrast, ASM]:\n",
    "            features.extend(glcm_feature[0])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def generate_tabular_data(self):\n",
    "        \"\"\"\n",
    "        Creates the tabular dataset, also normalizes the dataset, and\n",
    "        applies the stratified train_test_split\n",
    "        \"\"\"\n",
    "        for i, key in enumerate(self.classes):\n",
    "            for p in self.data_file_path[key]:\n",
    "                feature = self.generate_features(p)\n",
    "                self.data_y.extend([i])\n",
    "                self.data_x.extend([feature])\n",
    "        \n",
    "        self.data_x = np.array(self.data_x, dtype=np.float32)\n",
    "        self.data_y = np.array(self.data_y, dtype=np.float32)\n",
    "        \n",
    "        self.data_x = self.scaler.fit_transform(self.data_x)\n",
    "        \n",
    "        self.data_train_x, self.data_val_x, self.data_train_y, self.data_val_y = train_test_split(self.data_x, self.data_y, \n",
    "                                                                                                  train_size = 1.-self.val_fraction, \n",
    "                                                                                                  random_state = self.random_state,\n",
    "                                                                                                  stratify = self.data_y)\n",
    "    \n",
    "    def load_test_data(self):\n",
    "        \"\"\"\n",
    "        Loads the test data and port it to tabular form\n",
    "        \"\"\"\n",
    "        paths = [[os.path.join(self.test_root, x), float(x.split('.')[0])] for x in os.listdir(self.test_root)]\n",
    "        paths.sort(key = lambda x : x[1])\n",
    "        \n",
    "        self.test_data_path = paths\n",
    "        \n",
    "        for p in paths:\n",
    "            feature = self.generate_features(p[0])\n",
    "            self.data_test.extend([feature])\n",
    "        \n",
    "        self.data_test = self.scaler.transform(self.data_test)\n",
    "        self.data_test = np.array(self.data_test, dtype = np.float32)\n",
    "    \n",
    "    def __train__(self):\n",
    "        \"\"\"Returns Training Dataset\"\"\"\n",
    "        return self.data_train_x, self.data_train_y\n",
    "    \n",
    "    def __val__(self):\n",
    "        \"\"\"Returns Validation Dataset\"\"\"\n",
    "        return self.data_val_x, self.data_val_y\n",
    "    \n",
    "    def __test__(self):\n",
    "        \"\"\"Returns Test Features\"\"\"\n",
    "        return self.data_test\n",
    "    \n",
    "    def __pandas__(self):\n",
    "        \"\"\"Creates pandas dataframe for better visuakization\"\"\"\n",
    "        header = [f'F{i}' for i in range(self.data_x.shape[1])]\n",
    "        header.extend([\"Class\"])\n",
    "        local_data = np.concatenate((self.data_x, np.expand_dims(self.data_y, axis = 1)), axis=1)\n",
    "        df = pd.DataFrame(local_data, columns=header)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create `TabularDataset` class\n",
    "1. Handels the tabular datasets\n",
    "2. Subclass of `torch.Dataset`\n",
    "3. Implements indexable dataset for pytorch pipeline\n",
    "4. Handles data transforms internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T05:54:14.819848Z",
     "iopub.status.busy": "2022-02-14T05:54:14.819595Z",
     "iopub.status.idle": "2022-02-14T05:54:14.834025Z",
     "shell.execute_reply": "2022-02-14T05:54:14.833389Z",
     "shell.execute_reply.started": "2022-02-14T05:54:14.819818Z"
    }
   },
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, x, y, transform = None, target_transform = None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.x[idx, :]\n",
    "        target = self.y[idx]\n",
    "        target = np.array([target], dtype = np.float32) \n",
    "\n",
    "        if self.transform:\n",
    "            feature = self.transform(feature)\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "        \n",
    "        return torch.from_numpy(feature), torch.from_numpy(target)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the global training loop characterstics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T06:12:44.917052Z",
     "iopub.status.busy": "2022-02-14T06:12:44.916379Z",
     "iopub.status.idle": "2022-02-14T06:12:44.921143Z",
     "shell.execute_reply": "2022-02-14T06:12:44.920151Z",
     "shell.execute_reply.started": "2022-02-14T06:12:44.917014Z"
    }
   },
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "BATCH_SIZE = 30 # Batch size\n",
    "EPOCHS = 50 # Number of epochs (# complete data iteration)\n",
    "ITRS = 11 # Not used (calculated on the go)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate the `RavellingDataset` class instance\n",
    "> Note: This step will take time based on the system speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T06:33:27.638692Z",
     "iopub.status.busy": "2022-02-14T06:33:27.638395Z",
     "iopub.status.idle": "2022-02-14T06:34:10.779770Z",
     "shell.execute_reply": "2022-02-14T06:34:10.778810Z",
     "shell.execute_reply.started": "2022-02-14T06:33:27.638664Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = RavelingTrainDataset(val_fraction=0.2, random_state=51)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conver data to pandas dataframe and view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T06:34:10.781687Z",
     "iopub.status.busy": "2022-02-14T06:34:10.781411Z",
     "iopub.status.idle": "2022-02-14T06:34:10.814672Z",
     "shell.execute_reply": "2022-02-14T06:34:10.813507Z",
     "shell.execute_reply.started": "2022-02-14T06:34:10.781656Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset.__pandas__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the train dataset, and create the dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T06:34:10.816834Z",
     "iopub.status.busy": "2022-02-14T06:34:10.816231Z",
     "iopub.status.idle": "2022-02-14T06:34:10.823554Z",
     "shell.execute_reply": "2022-02-14T06:34:10.822579Z",
     "shell.execute_reply.started": "2022-02-14T06:34:10.816796Z"
    }
   },
   "outputs": [],
   "source": [
    "TrainX, TrainY = dataset.__train__()\n",
    "TrainDataset = TabularDataset(TrainX, TrainY)\n",
    "\n",
    "# Create torch dataloader for train dataset\n",
    "TrainDataloader = DataLoader(TrainDataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "TrainTestDataloader = DataLoader(TrainDataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "print(f'Train data : feature shape : {TrainX.shape} ; Labels shape : {TrainY.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the validation dataset, and create the loader utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T06:34:10.826466Z",
     "iopub.status.busy": "2022-02-14T06:34:10.826055Z",
     "iopub.status.idle": "2022-02-14T06:34:10.838289Z",
     "shell.execute_reply": "2022-02-14T06:34:10.837287Z",
     "shell.execute_reply.started": "2022-02-14T06:34:10.826418Z"
    }
   },
   "outputs": [],
   "source": [
    "ValX, ValY = dataset.__val__()\n",
    "ValDataset = TabularDataset(ValX, ValY)\n",
    "\n",
    "# Create torch dataloader for validation dataset\n",
    "ValDataloader = DataLoader(ValDataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "print(f'Validation data : feature shape : {ValX.shape} ; Labels shape : {ValY.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the test dataset along with its data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T06:34:10.840182Z",
     "iopub.status.busy": "2022-02-14T06:34:10.839571Z",
     "iopub.status.idle": "2022-02-14T06:34:10.853915Z",
     "shell.execute_reply": "2022-02-14T06:34:10.853253Z",
     "shell.execute_reply.started": "2022-02-14T06:34:10.840133Z"
    }
   },
   "outputs": [],
   "source": [
    "TestX = dataset.__test__()\n",
    "TestDataset = TabularDataset(TestX, np.zeros((len(TestX), )))\n",
    "\n",
    "# Create torch dataloader for test dataset\n",
    "TestDataloader = DataLoader(TestDataset, batch_size = BATCH_SIZE, shuffle = False)\n",
    "print(f'Test data shape : {TestX.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T06:34:10.855294Z",
     "iopub.status.busy": "2022-02-14T06:34:10.854965Z",
     "iopub.status.idle": "2022-02-14T06:34:11.029313Z",
     "shell.execute_reply": "2022-02-14T06:34:11.028406Z",
     "shell.execute_reply.started": "2022-02-14T06:34:10.855266Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's see the frequency of the train dataset classes\n",
    "plt.hist(TrainY, color = 'g', label = \"Train Dataset\")\n",
    "plt.gca().set(title='Frequency Histogram of Labels in Train Dataset', ylabel='Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T06:34:11.031242Z",
     "iopub.status.busy": "2022-02-14T06:34:11.030759Z",
     "iopub.status.idle": "2022-02-14T06:34:11.265143Z",
     "shell.execute_reply": "2022-02-14T06:34:11.264349Z",
     "shell.execute_reply.started": "2022-02-14T06:34:11.031195Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's see the frequency of the validation dataset classes\n",
    "plt.hist(ValY, color = 'orange', label = \"Validation Dataset\")\n",
    "plt.gca().set(title='Frequency Histogram of Labels in Validation Dataset', ylabel='Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "1. Using `torch.nn.Module` to define the model\n",
    "2. Using `nn.Sigmoid` classifier : _Binary classification_\n",
    "3. We will be using `L2` regularizer or more specifically an [equivalent known as `Weight Decay` regularization](https://towardsdatascience.com/weight-decay-l2-regularization-90a9e17713cd).\n",
    "4. For actiation function through out the neural network, we use `SELU` and upgraded version on `RELU` to avoid the data loss and still providing non-linearity\n",
    "> Note: The regularization will be enforced in the optimizer directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T06:34:11.267238Z",
     "iopub.status.busy": "2022-02-14T06:34:11.266441Z",
     "iopub.status.idle": "2022-02-14T06:34:11.279871Z",
     "shell.execute_reply": "2022-02-14T06:34:11.279035Z",
     "shell.execute_reply.started": "2022-02-14T06:34:11.267196Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create Model subclass to define the network\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features = 100):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define possible layers configuration\n",
    "        self.fc1 = nn.Linear(in_features, 150)\n",
    "        self.fc2 = nn.Linear(150, 90)\n",
    "        self.fc3 = nn.Linear(90, 70)\n",
    "        self.fc4 = nn.Linear(70, 50)\n",
    "        self.fc5 = nn.Linear(50, 30)\n",
    "        self.fc6 = nn.Linear(30, 20)\n",
    "        self.fc7 = nn.Linear(20, 10)\n",
    "        self.fc8 = nn.Linear(10, 5)\n",
    "        self.fc9 = nn.Linear(5, 1)\n",
    "        \n",
    "        # Define activations, classifier layer, \n",
    "        # and if required then regularizations\n",
    "        self.activation = nn.SELU() # Activations\n",
    "        self.classifier = nn.Sigmoid() # Classifier\n",
    "#         self.dropout = nn.Dropout(p=0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Function implements the `forward` pass of a network.\n",
    "        While training this will run with gradient enabled, to backprop,\n",
    "        otherwise while testing this is used with torch.no_grad() to infer on the query.\n",
    "        \"\"\"\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.activation(self.fc4(x))\n",
    "        x = self.activation(self.fc5(x))\n",
    "        x = self.activation(self.fc6(x))\n",
    "        x = self.activation(self.fc7(x))\n",
    "        x = self.activation(self.fc8(x))\n",
    "        x = self.classifier(self.fc9(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T06:34:11.281339Z",
     "iopub.status.busy": "2022-02-14T06:34:11.281135Z",
     "iopub.status.idle": "2022-02-14T06:34:11.296341Z",
     "shell.execute_reply": "2022-02-14T06:34:11.295458Z",
     "shell.execute_reply.started": "2022-02-14T06:34:11.281315Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'{device} device will be used.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model instance and move it to the device memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T06:34:11.299022Z",
     "iopub.status.busy": "2022-02-14T06:34:11.298549Z",
     "iopub.status.idle": "2022-02-14T06:34:11.308177Z",
     "shell.execute_reply": "2022-02-14T06:34:11.307586Z",
     "shell.execute_reply.started": "2022-02-14T06:34:11.298988Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transfer the model parameters and properties to selected device\n",
    "model = Model(TrainX.shape[1]).to(torch.device(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have a look at the model intrinsics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T06:34:11.309734Z",
     "iopub.status.busy": "2022-02-14T06:34:11.309434Z",
     "iopub.status.idle": "2022-02-14T06:34:11.326422Z",
     "shell.execute_reply": "2022-02-14T06:34:11.325591Z",
     "shell.execute_reply.started": "2022-02-14T06:34:11.309686Z"
    }
   },
   "outputs": [],
   "source": [
    "summary(model, (TrainX.shape[1],))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the helper functions for easy access to accuracy computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T06:34:11.328059Z",
     "iopub.status.busy": "2022-02-14T06:34:11.327772Z",
     "iopub.status.idle": "2022-02-14T06:34:11.336399Z",
     "shell.execute_reply": "2022-02-14T06:34:11.335506Z",
     "shell.execute_reply.started": "2022-02-14T06:34:11.328018Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_acc(dl):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in dl:\n",
    "            features, labels = data\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(features)\n",
    "\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            pivot = torch.tensor([0.5]).to(device)\n",
    "            value = torch.tensor([0.0]).to(device)\n",
    "            predicted = torch.heaviside(outputs.data-pivot, value)\n",
    "            \n",
    "            # print(predicted, labels)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return 100*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T06:34:30.285352Z",
     "iopub.status.busy": "2022-02-14T06:34:30.285076Z",
     "iopub.status.idle": "2022-02-14T06:34:30.293224Z",
     "shell.execute_reply": "2022-02-14T06:34:30.292650Z",
     "shell.execute_reply.started": "2022-02-14T06:34:30.285324Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_weights(layer):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        # Xavier normal initialization for all the layers\n",
    "        nn.init.xavier_normal_(layer.weight.data)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If `load_prev` is `True` the previous model `basemodel` will be loader if found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T06:34:31.290037Z",
     "iopub.status.busy": "2022-02-14T06:34:31.289647Z",
     "iopub.status.idle": "2022-02-14T06:34:31.294995Z",
     "shell.execute_reply": "2022-02-14T06:34:31.294216Z",
     "shell.execute_reply.started": "2022-02-14T06:34:31.290007Z"
    }
   },
   "outputs": [],
   "source": [
    "load_prev = False\n",
    "if load_prev:\n",
    "    model.load_state_dict(torch.load('./basemodel'))\n",
    "    print(model.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define te loss function, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T06:34:31.533193Z",
     "iopub.status.busy": "2022-02-14T06:34:31.532911Z",
     "iopub.status.idle": "2022-02-14T06:34:31.539008Z",
     "shell.execute_reply": "2022-02-14T06:34:31.538080Z",
     "shell.execute_reply.started": "2022-02-14T06:34:31.533163Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss() # Loss function\n",
    "params_list = model.parameters() # model parameters\n",
    "\n",
    "## We can apply custom learning rate or any other perameters to each layer, use the following:\n",
    "# params_list = [\n",
    "#     {'params': model.fc1.parameters(), 'lr': 0.01},\n",
    "#     {'params': model.fc1_1.parameters(), 'lr': 0.01},\n",
    "#     {'params': model.fc2.parameters(), 'lr': 0.005},\n",
    "#     {'params': model.fc2_2.parameters(), 'lr': 0.005},\n",
    "#     {'params': model.fc3.parameters(), 'lr': 0.001},\n",
    "#     {'params': model.fc3_3.parameters(), 'lr': 0.001},\n",
    "#     {'params': model.fc4.parameters(), 'lr': 0.005},\n",
    "#     {'params': model.fc4_4.parameters(), 'lr': 0.005},\n",
    "#     {'params': model.fc5.parameters(), 'lr': 0.001},\n",
    "#     {'params': model.fc5_5.parameters(), 'lr': 0.001},\n",
    "# ]\n",
    "optimizer = optim.AdamW(params_list, lr=0.0007, weight_decay=0.01) # Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T06:34:32.205682Z",
     "iopub.status.busy": "2022-02-14T06:34:32.205353Z",
     "iopub.status.idle": "2022-02-14T06:34:32.209643Z",
     "shell.execute_reply": "2022-02-14T06:34:32.208595Z",
     "shell.execute_reply.started": "2022-02-14T06:34:32.205653Z"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T06:34:32.363425Z",
     "iopub.status.busy": "2022-02-14T06:34:32.363001Z",
     "iopub.status.idle": "2022-02-14T06:34:37.074327Z",
     "shell.execute_reply": "2022-02-14T06:34:37.073466Z",
     "shell.execute_reply.started": "2022-02-14T06:34:32.363396Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "for epoch in range(EPOCHS):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    data_itr = tqdm(enumerate(TrainDataloader, 0))\n",
    "    for i, data in data_itr:\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(torch.device(device))\n",
    "        labels = labels.to(torch.device(device))\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:    # print every 100 mini-batches\n",
    "            accuracies.extend([[epoch + 1, i + 1, running_loss / 100, get_acc(TrainTestDataloader), get_acc(ValDataloader)]])\n",
    "            data_itr.set_description('[%d, %5d] loss: %.3f, train accuracy: %.3f, val accuracy: %.3f' %\n",
    "                  (accuracies[-1][0], accuracies[-1][1], accuracies[-1][2], accuracies[-1][3], accuracies[-1][4]))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T06:34:37.076122Z",
     "iopub.status.busy": "2022-02-14T06:34:37.075900Z",
     "iopub.status.idle": "2022-02-14T06:34:37.312065Z",
     "shell.execute_reply": "2022-02-14T06:34:37.311278Z",
     "shell.execute_reply.started": "2022-02-14T06:34:37.076094Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the training and validation accuracies throughout the training process\n",
    "accuracies = np.asarray(accuracies, dtype=np.float32)\n",
    "plt.plot(accuracies[:, 0], accuracies[:, 3], 'r-', label=\"Train Accuracies\")\n",
    "plt.plot(accuracies[:, 0], accuracies[:, 4], 'g-', label=\"Val Accuracies\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('Training vs Validation accuracies')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T06:32:26.848709Z",
     "iopub.status.busy": "2022-02-14T06:32:26.847992Z",
     "iopub.status.idle": "2022-02-14T06:32:26.874755Z",
     "shell.execute_reply": "2022-02-14T06:32:26.874045Z",
     "shell.execute_reply.started": "2022-02-14T06:32:26.848670Z"
    }
   },
   "outputs": [],
   "source": [
    "print(' Validation accuracy of the network: %f %%' % (\n",
    "    get_acc(ValDataloader)))\n",
    "print(' Train accuracy of the network: %f %%' % (\n",
    "    get_acc(TrainDataloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T06:21:03.916654Z",
     "iopub.status.busy": "2022-02-14T06:21:03.916357Z",
     "iopub.status.idle": "2022-02-14T06:21:03.923331Z",
     "shell.execute_reply": "2022-02-14T06:21:03.922451Z",
     "shell.execute_reply.started": "2022-02-14T06:21:03.916621Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./basemodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T06:21:31.666245Z",
     "iopub.status.busy": "2022-02-14T06:21:31.665851Z",
     "iopub.status.idle": "2022-02-14T06:21:31.683427Z",
     "shell.execute_reply": "2022-02-14T06:21:31.682590Z",
     "shell.execute_reply.started": "2022-02-14T06:21:31.666214Z"
    }
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "with torch.no_grad():\n",
    "    for data in TestDataloader:\n",
    "        features = data[0].to(device)\n",
    "\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = model(features)\n",
    "\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        pivot = torch.tensor([0.5]).to(device)\n",
    "        value = torch.tensor([0.0]).to(device)\n",
    "        predicted = torch.heaviside(outputs.data-pivot, value)\n",
    "        \n",
    "        result.extend(predicted.cpu().detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T06:22:59.324792Z",
     "iopub.status.busy": "2022-02-14T06:22:59.324469Z",
     "iopub.status.idle": "2022-02-14T06:22:59.331390Z",
     "shell.execute_reply": "2022-02-14T06:22:59.330600Z",
     "shell.execute_reply.started": "2022-02-14T06:22:59.324758Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert numeric classes to named one\n",
    "result_class = [[str(int(y[1]))+'.jpg', \"Raveling\" if (x == 1) else \"Non_raveling\"] for x, y in zip(result, dataset.test_data_path)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the results dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T06:23:23.473060Z",
     "iopub.status.busy": "2022-02-14T06:23:23.472807Z",
     "iopub.status.idle": "2022-02-14T06:23:23.483923Z",
     "shell.execute_reply": "2022-02-14T06:23:23.483082Z",
     "shell.execute_reply.started": "2022-02-14T06:23:23.473033Z"
    }
   },
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame(result_class, columns = ['filename', 'class'])\n",
    "df_result.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-14T06:23:38.090193Z",
     "iopub.status.busy": "2022-02-14T06:23:38.089426Z",
     "iopub.status.idle": "2022-02-14T06:23:38.095935Z",
     "shell.execute_reply": "2022-02-14T06:23:38.095020Z",
     "shell.execute_reply.started": "2022-02-14T06:23:38.090153Z"
    }
   },
   "outputs": [],
   "source": [
    "## Save to csv file, and submit\n",
    "df_result.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
